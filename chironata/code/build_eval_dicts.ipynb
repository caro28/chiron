{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2123425a-0658-4d31-8858-8e44dd272cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import json\n",
    "from functions import load_txt_as_lst, read_alignments\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f2a17-abdd-4056-83cc-aa0fe0821031",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# TEST: Run labse on every sents file in input directory, then write to scratch\n",
    "Moved to .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b65a0be-13c1-4add-b26f-e0f2845887a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urn:cts:greekLit:tlg0001.tlg001\n"
     ]
    }
   ],
   "source": [
    "sents_dir = \"/home/craig.car/repos/chiron/chironata/data/src_data/\"\n",
    "for sents_path in glob.iglob(sents_dir+\"*.sents\"):\n",
    "    print(os.path.splitext(os.path.basename(sents_path))[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81808dad-1338-4e07-b12d-24db2ad1baf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeschylus_1863\n"
     ]
    }
   ],
   "source": [
    "sents_dir = \"/home/craig.car/repos/chiron/chironata/data/french_trans-dev/\"\n",
    "for sents_path in glob.iglob(sents_dir+\"*.sents\"):\n",
    "    print(os.path.splitext(os.path.basename(sents_path))[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a4e7dda-bb69-4821-af31-a9af46c89484",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_dir = \"/home/craig.car/repos/chiron/chironata/data/src_data/\"\n",
    "dir_out = \"/scratch/craig.car/sents_embeds/src_embeds/\"\n",
    "    \n",
    "for sents_path in glob.iglob(sents_dir+\"*.sents\"):\n",
    "    filename = os.path.splitext(os.path.basename(sents_path))[0]\n",
    "    if os.path.isfile(dir_out+filename+\".emb\") == False:\n",
    "        print(filename)\n",
    "        txt_lst = load_txt_as_lst(sents_path)\n",
    "        # labse_embeds = build_embeddings_huggingface(txt_lst, model)\n",
    "        print(\"built embeds\")\n",
    "        path_out = dir_out+filename+\".emb\"\n",
    "        # write_to_file(labse_embeds,path_out)\n",
    "        print(path_out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390d1da-764b-46df-b7c0-9b7987fa5f9d",
   "metadata": {},
   "source": [
    "# Run sentence retrieval and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eb418ed-d72a-4f4d-b6e8-2f1d2c244787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim_matrix_cosine(src_embeds, tgt_embeds):\n",
    "    # using cosine sim\n",
    "    return cosine_similarity(src_embeds, tgt_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e4a2877-fdb4-4adb-b7bf-790e33c309bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lookup table\n",
    "lookup_path = \"/home/craig.car/repos/chiron/chironata/data/cts_lookup_table.json\"\n",
    "with open(lookup_path) as f:\n",
    "    lookup = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aae5af8-b432-4691-9ad4-6455e0c98ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demosthenes_1856_1',\n",
       " 'demosthenes_2_1861',\n",
       " 'demosthenes_1_1863',\n",
       " 'demosthenes_aeschynes_1861']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup[\"urn:cts:greekLit:tlg0026.tlg003\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87c321aa-eb89-404b-bf46-20a2f211c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709\n"
     ]
    }
   ],
   "source": [
    "fr_embed_dir = \"/scratch/craig.car/sents_embeds/fr_embeds/\"\n",
    "de_embed_dir = \"/scratch/craig.car/sents_embeds/de_embeds/\"\n",
    "en_embed_dir = \"/scratch/craig.car/sents_embeds/en_embeds/\"\n",
    "it_embed_dir = \"/scratch/craig.car/sents_embeds/it_embeds/\"\n",
    "align_dir = \"/home/craig.car/repos/chiron/chironata/data/alignments_rslts/\"\n",
    "text_dicts_dir = \"/home/craig.car/repos/chiron/chironata/sentence_aligned_texts/\"\n",
    "eval_data_dir = \"/home/craig.car/repos/chiron/chironata/eval_datafiles/\"\n",
    "\n",
    "dim = 768\n",
    "\n",
    "src_embeds_dir = \"/scratch/craig.car/sents_embeds/src_embeds/\"\n",
    "corpus_len = len([name for name in os.listdir(src_embeds_dir) if os.path.isfile(os.path.join(src_embeds_dir, name))])\n",
    "print(corpus_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a162abcd-56a2-41e3-a1b8-55f616f86a42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on /scratch/craig.car/sents_embeds/src_embeds/urn:cts:latinLit:stoa0023.stoa001.emb\n",
      "urn:cts:latinLit:stoa0023.stoa001\n",
      "['ammianusMarcellinus_1894', 'stoa0023.stoa001.ogl-eng1.xml']\n",
      "working on ammianusMarcellinus_1894 translation\n",
      "working on stoa0023.stoa001.ogl-eng1 translation\n",
      "tgt lang is en\n",
      "top 10 is 4512\n",
      "top 1 by transl is 0.6826359014470083\n",
      "top 5 by transl is 0.8335940555338287\n",
      "top 10 by transl is 0.8822839264763395\n",
      "text top 10 is 0.44114196323816973\n",
      "top1 corpus is 0.0004814075468596673\n",
      "top5 corpus is 0.0005878660476261134\n",
      "top10 corpus is 0.0006222030511116639\n"
     ]
    }
   ],
   "source": [
    "src_embeds_dir = \"/scratch/craig.car/sents_embeds/src_embeds/\"\n",
    "corpus_len = len([name for name in os.listdir(src_embeds_dir) if os.path.isfile(os.path.join(src_embeds_dir, name))])\n",
    "corpus_top1 = 0\n",
    "corpus_top5 = 0\n",
    "corpus_top10 = 0\n",
    "\n",
    "for src_embed_path in [\"/scratch/craig.car/sents_embeds/src_embeds/urn:cts:latinLit:stoa0023.stoa001.emb\"]:\n",
    "# for src_embed_path in glob.iglob(src_embeds_dir+\"*.emb\"):\n",
    "    src_eval_data_dict = {}\n",
    "    print(f\"working on {src_embed_path}\")\n",
    "    text_top1 = 0\n",
    "    text_top5 = 0\n",
    "    text_top10 = 0\n",
    "    # get embeds and resize\n",
    "    src_embeddings = np.fromfile(src_embed_path, dtype=np.float32, count=-1)\n",
    "    src_embeddings.resize(src_embeddings.shape[0] // dim, dim)\n",
    "    # print(f\"shape of src embeds {src_embeddings.shape}\")\n",
    "    # save num of src sents to src_eval data dict\n",
    "    src_eval_data_dict[\"num_src_sents\"] = src_embeddings.shape[0]\n",
    "    # get translations\n",
    "    ctsurn = os.path.splitext(os.path.basename(src_embed_path))[0]\n",
    "    print(ctsurn)\n",
    "    translations = lookup[ctsurn]\n",
    "    print(translations)\n",
    "    # for score denominators (reduce if encounter a translation with no rslts)\n",
    "    num_transl = len(translations)\n",
    "    # get text data dict\n",
    "    text_json_path = text_dicts_dir+ctsurn+\".json\"\n",
    "    with open(text_json_path) as f:\n",
    "        text_json = json.load(f)\n",
    "    for tgt_text in translations:\n",
    "        if tgt_text.endswith(\".xml\"):\n",
    "            tgt_text = tgt_text.split(\".xml\")[0]\n",
    "        transl_eval_dict = {}\n",
    "        print(f\"working on {tgt_text} translation\")\n",
    "        # count scores per translation\n",
    "        top_1 = 0\n",
    "        top_5 = 0\n",
    "        top_10 = 0\n",
    "        num_nulls = 0\n",
    "        # get embed file using lang in text_json\n",
    "        if tgt_text not in text_json.keys():\n",
    "            continue\n",
    "        else:\n",
    "            tgt_lang = text_json[tgt_text][\"tgt_lang\"]\n",
    "            transl_eval_dict[\"tgt_lang\"] = tgt_lang\n",
    "            print(f\"tgt lang is {tgt_lang}\")\n",
    "            # if os.path.isfile(de_embed_dir+tgt_text+\".emb\"):\n",
    "            if tgt_lang == \"de\":\n",
    "                tgt_embed = de_embed_dir+tgt_text+\".emb\"\n",
    "            # elif os.path.isfile(en_embed_dir+tgt_text+\".emb\"):\n",
    "            elif tgt_lang == \"en\":\n",
    "                tgt_embed = en_embed_dir+tgt_text+\".emb\"\n",
    "            # elif os.path.isfile(it_embed_dir+tgt_text+\".emb\"):\n",
    "            elif tgt_lang == \"it\":\n",
    "                tgt_embed = it_embed_dir+tgt_text+\".emb\"\n",
    "            # elif os.path.isfile(fr_embed_dir+tgt_text+\".emb\"):\n",
    "            elif tgt_lang == \"fr\":\n",
    "                tgt_embed = fr_embed_dir+tgt_text+\".emb\"\n",
    "            # get tgt embeds\n",
    "            tgt_embeddings = np.fromfile(tgt_embed, dtype=np.float32, count=-1)\n",
    "            tgt_embeddings.resize(tgt_embeddings.shape[0] // dim, dim)\n",
    "            # print(f\"shape of tgt embeds {tgt_embeddings.shape}\")\n",
    "            # build sim matrix\n",
    "            sim_matrix = compute_sim_matrix_cosine(src_embeddings, tgt_embeddings)\n",
    "            # iterate through src sents, compare sim score (src_sent,pred_sent) to top 1,5,10 scores\n",
    "            for src_idx, src_sent_vector in enumerate(sim_matrix):\n",
    "                # print(f\"len of src sent vector is {len(src_sent_vector)}, src idx is {src_idx}\")\n",
    "                # skip translations that were deleted\n",
    "                if tgt_text not in text_json.keys():\n",
    "                    # back out from len of translations\n",
    "                    num_transl -= 1\n",
    "                else:\n",
    "                    pred_sents_idx = text_json[tgt_text][\"aligns_idx\"][str(src_idx)]\n",
    "                    # print(pred_sents_idx)\n",
    "                    # sort vector of sim scores\n",
    "                    sorted_sim_vec = np.flip(np.sort(src_sent_vector))\n",
    "                    # check for each pred_sent in pred_sents\n",
    "                    for tgt_sent_idx in pred_sents_idx:\n",
    "                        if tgt_sent_idx == \"null\":\n",
    "                            num_nulls += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            tgt_sent_simscore = src_sent_vector[tgt_sent_idx]\n",
    "                            if tgt_sent_simscore >= sorted_sim_vec[0]:\n",
    "                                # print(\"have top 1\")\n",
    "                                top_1 += 1\n",
    "                                top_5 += 1\n",
    "                                top_10 += 1\n",
    "                                continue\n",
    "                            elif tgt_sent_simscore >= sorted_sim_vec[4]:\n",
    "                                # print(\"have top 5\")\n",
    "                                top_5 += 1\n",
    "                                top_10 += 1\n",
    "                                continue    \n",
    "                            elif tgt_sent_simscore >= sorted_sim_vec[9]:\n",
    "                                top_10 += 1\n",
    "                                # print(\"have top 10\")\n",
    "                                continue\n",
    "        \n",
    "        # how many src sents have a pred that's in top 1, top 5, top 10? Don't back out num_nulls from denominator bc no src sent should be aligned to null\n",
    "        top_1_transl = top_1/sim_matrix.shape[0]\n",
    "        top_5_transl = top_5/sim_matrix.shape[0]\n",
    "        top_10_transl = top_10/sim_matrix.shape[0]\n",
    "        print(f\"top 10 is {top_10}\")\n",
    "        print(f\"top 1 by transl is {top_1_transl}\")\n",
    "        print(f\"top 5 by transl is {top_5_transl}\")\n",
    "        print(f\"top 10 by transl is {top_10_transl}\")\n",
    "        transl_eval_dict[\"transl_top1\"] = top_1_transl\n",
    "        transl_eval_dict[\"transl_top5\"] = top_5_transl\n",
    "        transl_eval_dict[\"transl_top10\"] = top_10_transl\n",
    "        transl_eval_dict[\"num_nulls\"] = num_nulls\n",
    "        transl_eval_dict[\"num_tgt_sents\"] = tgt_embeddings.shape[0]\n",
    "        # add to src_eval_data_dict\n",
    "        src_eval_data_dict[tgt_text] = transl_eval_dict\n",
    "        \n",
    "        # add to text-level average\n",
    "        text_top1 += top_1_transl/num_transl\n",
    "        text_top5 += top_5_transl/num_transl\n",
    "        text_top10 += top_10_transl/num_transl\n",
    "        print(f\"text top 10 is {text_top10}\")\n",
    "        \n",
    "    src_eval_data_dict[\"num_translations\"] = num_transl\n",
    "    src_eval_data_dict[\"text_top1\"] = text_top1\n",
    "    src_eval_data_dict[\"text_top5\"] = text_top5\n",
    "    src_eval_data_dict[\"text_top10\"] = text_top10\n",
    "    \n",
    "    # write to json\n",
    "    path_out = eval_data_dir+ctsurn+\".json\"\n",
    "    with open(path_out, 'w') as fp:\n",
    "        json.dump(src_eval_data_dict, fp, ensure_ascii=False)\n",
    "\n",
    "    # add to corpus-level average\n",
    "    corpus_top1 += text_top1/corpus_len\n",
    "    corpus_top5 += text_top5/corpus_len\n",
    "    corpus_top10 += text_top10/corpus_len\n",
    "\n",
    "print(f\"top1 corpus is {corpus_top1}\")\n",
    "print(f\"top5 corpus is {corpus_top5}\")\n",
    "print(f\"top10 corpus is {corpus_top10}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406dd87b-01fc-4793-b527-e14e184ec295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_src_sents': 1738,\n",
       " 'apolloniusRhodius_1_1791': {'tgt_lang': 'it',\n",
       "  'transl_top1': 0.0023014959723820483,\n",
       "  'transl_top5': 0.013233601841196778,\n",
       "  'transl_top10': 0.023590333716915997,\n",
       "  'num_nulls': 0,\n",
       "  'num_tgt_sents': 2980},\n",
       " 'tlg0001.tlg001.opp-eng1': {'tgt_lang': 'en',\n",
       "  'transl_top1': 0.34177215189873417,\n",
       "  'transl_top5': 0.5270425776754891,\n",
       "  'transl_top10': 0.6052934407364787,\n",
       "  'num_nulls': 0,\n",
       "  'num_tgt_sents': 3248},\n",
       " 'apolloniusRhodius_1892': {'tgt_lang': 'fr',\n",
       "  'transl_top1': 0.19275028768699654,\n",
       "  'transl_top5': 0.3486766398158803,\n",
       "  'transl_top10': 0.4315304948216341,\n",
       "  'num_nulls': 0,\n",
       "  'num_tgt_sents': 12099},\n",
       " 'num_translations': 3,\n",
       " 'text_top1': 0.17894131185270423,\n",
       " 'text_top5': 0.29631760644418875,\n",
       " 'text_top10': 0.35347142309167623}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_eval_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64ae12ad-56f4-4388-b01d-4fee25883dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1 corpus is 0.21721537007984332\n",
      "top5 corpus is 0.3277851981538398\n",
      "top10 corpus is 0.38836388697782265\n"
     ]
    }
   ],
   "source": [
    "print(f\"top1 corpus is {corpus_top1}\")\n",
    "print(f\"top5 corpus is {corpus_top5}\")\n",
    "print(f\"top10 corpus is {corpus_top10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecbcde3-4a65-4093-9889-68efb3eef3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
